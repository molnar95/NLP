{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data manipulating:\n",
    "import unicodedata\n",
    "import re \n",
    "from datetime import datetime\n",
    "#from langdetect import detect\n",
    "\n",
    "# NLP packages:\n",
    "import hu_core_ud_lg as hu\n",
    "nlp2 = hu.load()\n",
    "import spacy\n",
    "\n",
    "# Tokenization:\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.hu import Hungarian\n",
    "nlp1 = Hungarian()\n",
    "\n",
    "# Faster than Spacy:\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the created hungarian stop_words.txt:\n",
    "with open(\"stop_words.txt\", \"r\", encoding='utf-8') as f:\n",
    "    stop = [i for line in f for i in line.split('\\n')]\n",
    "    stop = list(filter(None, stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe:\n",
    "df_index = pd.read_csv(\"C:/Users/molna/Desktop/Projektek/Python oktatás/python_bevez_oktatas/data/2020.csv\", sep=\"%%\", encoding=\"utf-8\", header=0)\n",
    "len(df_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    \"\"\"\n",
    "    Transforming the imported Index.hu dataframes:\n",
    "    - Header cleaning,\n",
    "    - Merge columns,\n",
    "    - Regex,\n",
    "    - Drop wrong rows,\n",
    "    - Drop english articles, \n",
    "    - Create day column.\n",
    "    \"\"\"\n",
    "    df.columns = df.columns.str.replace('\"', '')\n",
    "\n",
    "    for column in df:\n",
    "        df[column] = df[column].astype(str).str.replace('\"', '')\n",
    "\n",
    "    df = df[(df['szoveg'] != 'NA')]\n",
    "    df = df[df['szoveg'] != ' '] \n",
    "    df = df[df['szoveg'] != '  '] \n",
    "    df = df[~df['tag'].str.contains('Napirajz')]\n",
    "    df = df[df['cim'] != 'NA']\n",
    "\n",
    "    df = df[~df.duplicated(['cim', 'szoveg'], keep = 'last')]\n",
    "\n",
    "    df['merged'] = df['head'] + df['szoveg']\n",
    "    df['merged'] = df['merged'].astype(str).str.replace('NA', '')\n",
    "\n",
    "    df[\"merged\"] = df[\"merged\"].apply(lambda x: re.sub('Common.charts.register.*;', '', x,flags=re.DOTALL))\n",
    "\n",
    "    df['merged'] = df['merged'].str.replace('\\xa0', ' ')\n",
    "    df['merged'] = df['merged'].str.replace('Ne maradjon le semmiről! Facebook', '')\n",
    "\n",
    "    '''\n",
    "    nyelv=[]\n",
    "    for i in range(0,(len(df))):\n",
    "        if len(df['merged'].iloc[i])>2 :\n",
    "            nyelv.append(detect(df['merged'].iloc[i]))\n",
    "        else: \n",
    "            nyelv.append(np.NaN)    \n",
    "    \n",
    "    df = df[df['nyelv'] == 'hu'] \n",
    "    '''\n",
    "    df['datum'] = df['datum'].str.split('Módosítva:').str[0]\n",
    "    df['nap'] = pd.to_datetime(df['datum']).dt.date\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_cleaning(df):\n",
    "    \"\"\"\n",
    "    Transforming title column for the NLP:\n",
    "    - Remove punctuations,\n",
    "    - Lowercasing,\n",
    "    - Remove stopwords.\n",
    "    \"\"\"\n",
    "    \n",
    "    df['cim_cleaned'] = df['cim'].str.replace('-', ' ')\n",
    "    df['cim_cleaned'] = df['cim_cleaned'].str.replace('[^\\w\\s]', '')\n",
    "    df['cim_cleaned'] = df['cim_cleaned'].str.replace('[0-9]', ' ')\n",
    "    \n",
    "    df['cim_cleaned'] = df['cim_cleaned'].str.lower()\n",
    "    df['cim_cleaned'] = df['cim_cleaned'].str.replace('\\xa0', ' ')\n",
    "\n",
    "    df['cim_cleaned'] = df['cim_cleaned'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_cleaning(df):\n",
    "    \"\"\"\n",
    "    Transforming szoveg (merged) column for the NLP:\n",
    "    - Remove punctuations,\n",
    "    - Lowercasing,\n",
    "    - Remove stopwords.\n",
    "    \"\"\"\n",
    "    \n",
    "    df['szoveg_cleaned'] = df['merged'].str.replace('-', ' ')\n",
    "    df['szoveg_cleaned'] = df['szoveg_cleaned'].str.replace('[^\\w\\s]', ' ')\n",
    "    df['szoveg_cleaned'] = df['szoveg_cleaned'].str.replace('[0-9]', ' ')\n",
    "    \n",
    "    df['szoveg_cleaned'] = df['szoveg_cleaned'].str.lower()\n",
    "    df['szoveg_cleaned'] = df['szoveg_cleaned'].str.replace('\\xa0', ' ')\n",
    "\n",
    "    df['szoveg_cleaned'] = df['szoveg_cleaned'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def desc_stat(df):\n",
    "    \"\"\"\n",
    "    #Calculating basic statistics for title and article, after cleaning.\n",
    "    \"\"\"\n",
    "\n",
    "    # descriptive statistics for the titles:\n",
    "    df['cim_word_cnt_wth_stop'] = df['cim_without_punct'].apply(lambda x: len(str(x).split(' ')))\n",
    "    df['cim_word_cnt_wthout_stop'] = df['cim_without_stop'].apply(lambda x: len(str(x).split(' ')))\n",
    "    df['cim_stop_cnt'] = df['cim_without_punct'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "\n",
    "    # descriptive statistics for the articles:\n",
    "    df['merged_word_cnt_wth_stop'] = df['merged_without_punct'].apply(lambda x: len(str(x).split(' ')))\n",
    "    df['merged_word_cnt_wth_stop'] = df['merged_without_stop'].apply(lambda x: len(str(x).split(' ')))\n",
    "    df['merged_stop_cnt'] = df['merged_without_punct'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "\n",
    "    return df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function calls:\n",
    "df_index = data_cleaning(df_index)\n",
    "df_index = title_cleaning(df_index)\n",
    "df_index = merged_cleaning(df_index)\n",
    "#df_index = desc_stat(df_index)\n",
    "\n",
    "print(df_index.columns)\n",
    "print(len(df_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing - Hungarian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Spacy:\n",
    "'''\n",
    "def tokenizer(df):\n",
    "    tokenizer = Tokenizer(nlp1.vocab)\n",
    "    df['cim_tokens'] = [tokenizer (i) for i in df['cim_without_stop']]\n",
    "    df['merged_tokens'] = [tokenizer (i) for i in df['merged_without_stop']]\n",
    "\n",
    "    return df \n",
    "\n",
    "df_index = tokenizer(df_index)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With NLTK:\n",
    "df_index[\"merged_tokens\"] = df_index[\"szoveg_cleaned\"].apply(lambda x: tknzr.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def lemmatizer(df):\n",
    "    \"\"\"\n",
    "    #Lemmatize the article's tokens.\n",
    "    \"\"\"\n",
    "    lemmas = []\n",
    "    for j in range(0, len(df['merged_tokens'])):\n",
    "        lemma = [i.lemma_ for i in nlp2(str(df['merged_tokens'][j]))]\n",
    "        lemma = str(lemma)\n",
    "        lemma = re.sub(\"'\", '', lemma)\n",
    "        lemma = re.sub(\"\\[|\\]\",\" \", lemma)\n",
    "        #a = re.sub(' ,', '', a)\n",
    "        #a = re.sub(' ', '', a)\n",
    "        lemmas.append(lemma)\n",
    "\n",
    "    df['merged_lemmas'] = lemmas\n",
    "    df['merged_lemmas'] = df['merged_lemmas'].apply(lambda x: x.split(\",\"))\n",
    "\n",
    "    return df\n",
    "    \n",
    "df_index = lemmatizer(df_index) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index['merged_lemmas'] = df_index['szoveg_cleaned'].apply(lambda x: [i.lemma_ for i in nlp2(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index[['merged_tokens','merged_lemmas']][0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def ner_rec(df):\n",
    "        \"\"\"\n",
    "        Named Entity Recognition on the article's tokens.\n",
    "        \"\"\"\n",
    "        ners = []\n",
    "        for j in range(0, len(df['merged_tokens'])):\n",
    "            ner = [i.pos_ for i in nlp2(str(df['merged_tokens'][j]))]\n",
    "            ner = str(ner)\n",
    "            ner = re.sub(\"'\", '', ner)\n",
    "            ner = re.sub(\"\\[|\\]\",\" \", ner)\n",
    "            #a = re.sub('  ,', '', a)\n",
    "            ners.append(ner)\n",
    "\n",
    "        df['merged_ners'] = ners\n",
    "        df['merged_ners'] = df['merged_ners'].apply(lambda x: x.split(\",\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "df_index = ner_rec(df_index)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index['merged_ners'] = df_index['szoveg_cleaned'].apply(lambda x: [i.pos_ for i in nlp2(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index[['merged_tokens','merged_lemmas', 'merged_ners']][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export dataframe to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index.to_pickle('index_2020_nlp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle('2016_index.pkl')\n",
    "print(len(df2))\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['cim']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pickle dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('minta_nlp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## További adattisztítás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b\n",
    "\n",
    "# https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# szót nézi\n",
    "CBOW_model = Word2Vec(df_index[\"merged_lemmas\"], min_count=5, workers=3, window=9, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_model.most_similar(positive= ['koronavírus', 'covid'], negative=[], topn=10, restrict_vocab=None, indexer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_model.save(\"word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model = Word2Vec(df[\"merged_lemmas\"], min_count=5, workers=3, window=9, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model.most_similar(positive=[' politika'], negative=[], topn=100, restrict_vocab=None, indexer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kell = [' szabadidősport',\n",
    "        ' versenysport',\n",
    "        ' utánpótlásprogram',\n",
    "        ' mob',\n",
    "        ' versenyrendszer',\n",
    "        ' sportszövetség',\n",
    "        ' foci',\n",
    "        ' sportirányítás',\n",
    "        ' sportágfejlesztési',\n",
    "        ' sikersportág',\n",
    "        ' bozsikprogram',\n",
    "        ' futball',\n",
    "        ' utánpótlásnevelés',\n",
    "        ' utánpótlásbázis',\n",
    "        ' ökölvívás',\n",
    "        ' utánpótlásképzés',\n",
    "        ' parasport',\n",
    "        ' sportállamtitkárság',\n",
    "        ' mobfőtitkár',\n",
    "        ' jégkorongszövetség',\n",
    "        ' utánpótlásnevelő',\n",
    "        ' csapatsportág',\n",
    "        ' látványsportág',\n",
    "        ' héraklészprogram',\n",
    "        ' labdarúgás',\n",
    "        ' kajakkenu',\n",
    "        ' utánpótlás',\n",
    "        ' mefs',\n",
    "        ' dzsúdó',\n",
    "        ' társaságiadókedvezmény',\n",
    "        ' látványcsapatsport',\n",
    "        ' birkózás',\n",
    "        ' öttusa',\n",
    "        ' labdarúgószövetség',\n",
    "        ' látványsport',\n",
    "        ' küzdősport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_model.most_similar(positive= kell, negative=[], topn=100, restrict_vocab=None, indexer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def environment(mire,mennyi_kozel,mennyi_tavol):\n",
    "    neighbours_neighbours=[]\n",
    "    for i in range(0,mennyi_kozel):\n",
    "        a=skipgram_model.most_similar(positive=mire, negative=[], topn=mennyi_kozel, restrict_vocab=None, indexer=None)[i][0]\n",
    "        lista=skipgram_model.most_similar(positive=[a], negative=[], topn=mennyi_tavol, restrict_vocab=None, indexer=None)\n",
    "        valami=[]\n",
    "        for j in range(0,len(lista)):\n",
    "            valami.append(lista[j][0])\n",
    "    neighbours_neighbours.append(valami)\n",
    "    return(neighbours_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(environment(' sport', 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model.most_similar(positive=[], negative=[' korrupció'], topn=10, restrict_vocab=None, indexer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model.most_similar(positive=[' korrupció'], negative=[], topn=10, restrict_vocab=None, indexer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_model.most_similar(positive=[' sport', ' tao'], negative=[], topn=20, restrict_vocab=None, indexer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_model.most_similar(positive=[' tao'], negative=[], topn=20, restrict_vocab=None, indexer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_model.most_similar(positive=[' sport', ' politika'], negative=[' szabadidő'], topn=20, restrict_vocab=None, indexer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_model.most_similar(positive=[' origo'], negative=[], topn=20, restrict_vocab=None, indexer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = CBOW_model.most_similar(positive=[' origo'], negative=[], topn=20, restrict_vocab=None, indexer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "szolista = []\n",
    "for i in range(0, len(a)):\n",
    "    szolista.append(a[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "szolista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = []\n",
    "for i in range(0, len(szolista)):\n",
    "    lista.append(re.sub(' ', '', szolista[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['lemmas'].astype(str).str.contains(r'\\b(?:{})\\b'.format('|'.join(lista)))] # szűrés str.contains() fv-el."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
